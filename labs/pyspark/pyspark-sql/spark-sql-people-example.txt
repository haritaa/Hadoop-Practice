
df = spark.read.json("file:///home/ubuntu/danskeit_hadoop-pyspark/labs/dataset/customer/customer.json")

df.show()
df.printSchema()

df.select("name").show()

df.select(df("name"), df("age") + 1).show()
df.filter(df("age") > 21).show()
df.groupBy("age").count().show()

df.registerTempTable("people")

teenagers = sqlContext.sql("SELECT name, age FROM people WHERE age >= 13 AND age <= 19")
teenagers.show()	// Displays records
teenagers.count()	// Displays count

teenagers.collect()	// Collects data from Dataframe to Array
teenagers.collect().foreach(println)	// Prints data available in the Dataframe

#Write data in parquet format to hdfs
teenagers.select("name", "age").write.format("parquet").save("/home/ubuntu/spark/parquet")

#Write data in parquet format to local file system
teenagers.select("name", "age").write.format("parquet").save("file:///home/ubuntu/spark/teenagers.parquet")

#Read the data in parquet from hdfs
newDf = spark.read.format("parquet").load("/home/ubuntu/spark/parquet")

#Print details
newDf.show()

#Write data in orc format to hdfs 
newDf.select("name", "age").write.format("orc").save("/home/ubuntu/spark/orc/teenagers.orc")

#Start Spark Shell with Avro dependency
pyspark --packages org.apache.spark:spark-avro_2.12:3.0.1

#Read the data in parquet from hdfs
newDf = spark.read.format("parquet").load("/home/ubuntu/spark/parquet")

#Write data in avro format
newDf.select("name", "age").write.format("avro").save("/home/ubuntu/spark/avro/teenagers.avro")

#Read data in avro format
spark.read.format("avro").load("/home/ubuntu/spark/avro/teenagers.avro")
